# Twitter-Data-ETL-Pipeline-with-Apache-Airflow-on-AWS
# Key Features

Data Extraction: Uses Tweepy to interact with the Twitter API and extract tweets.

Data Transformation: Leverages Python's Pandas library to clean and prepare data.

Data Loading: Stores the transformed data in AWS S3 for persistence and further analysis.

Automation and Orchestration: Utilizes Apache Airflow running on AWS EC2 for scheduling and automating the ETL pipeline.

# Technologies Used

Python: For scripting and data manipulation.

Apache Airflow: For workflow orchestration.

AWS EC2: As the computing environment to host Airflow.

AWS S3: For data storage.

Twitter API: For sourcing real-time data.

# Setup and Installation

Prerequisites

AWS account with access to EC2, S3, and IAM roles.

Twitter Developer account and API credentials.

Local machine with Python 3.5 or above installed.
